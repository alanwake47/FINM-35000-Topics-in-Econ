{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin \n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 TOPICS IN ECONOMICS - GROUP 2\n",
    "\n",
    "<span style=\"color:blue\">Aman Krishna </span> <br>\n",
    "<br>\n",
    "<span style=\"color:#406A5F\">Tim Taylor </span> <br>\n",
    "<br>\n",
    "<span style=\"color:purple\">Yazmin Ramirez Delgado </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Measuring Hawkish/Dovish Tone of FOMC Statements (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \n",
    "\n",
    "Scrape the text of the FOMC statements from January 2000 to present. You will need to use https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm for 2018-2023 and https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm for 2000-2017. See Appendix A for hints on scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) How many statements do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements:  196\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import statistics\n",
    "\n",
    "# Function to scrape FOMC statements from a given URL (NEW)\n",
    "def scrape_statements_new(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    statements = []\n",
    "\n",
    "    # Find and collect the links to FOMC statements\n",
    "    links = soup.find_all('a', href=True)\n",
    "    for link in links:\n",
    "        if \"fomcpresconf\" in link['href']:\n",
    "            statement_url = urljoin(url, link['href'])  # Combine with base URL\n",
    "            statements.append(statement_url)\n",
    "\n",
    "    return statements\n",
    "\n",
    "# Function to scrape older FOMC statments which are kind of archived (OLD)\n",
    "def scrape_statements_old(url=\"https://www.federalreserve.gov/monetarypolicy/fomchistorical\", year=2017):\n",
    "    url = url + str(year) + \".htm\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    statements = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link.text.lower() == \"statement\":\n",
    "            statement_url = urljoin(url, link['href'])\n",
    "            statements.append(statement_url)\n",
    "            \n",
    "    return statements\n",
    "\n",
    "# Function to calculate the number of words in a text\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Function to calculate summary statistics\n",
    "def calculate_summary_statistics(word_counts):\n",
    "    mean = statistics.mean(word_counts)\n",
    "    stdev = statistics.stdev(word_counts)\n",
    "    minimum = min(word_counts)\n",
    "    q1 = statistics.quantiles(word_counts, n=4)[0]\n",
    "    median = statistics.median(word_counts)\n",
    "    q3 = statistics.quantiles(word_counts, n=4)[2]\n",
    "    maximum = max(word_counts)\n",
    "    \n",
    "    return mean, stdev, minimum, q1, median, q3, maximum\n",
    "\n",
    "# Define the base URLs for the different time periods\n",
    "base_url_2000_2017 = \"https://www.federalreserve.gov/monetarypolicy/fomchistorical\"\n",
    "base_url_2018_2023 = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "\n",
    "# Scrape statements for 2018-2023\n",
    "statements_2018_2023 = scrape_statements_new(base_url_2018_2023)\n",
    "statements_2000_2017 = []\n",
    "for year in range(2000, 2018):\n",
    "    statements_2000_2017 += scrape_statements_old(base_url_2000_2017, year)\n",
    "\n",
    "# Combine the statements from both time periods\n",
    "all_statements = statements_2000_2017 + statements_2018_2023\n",
    "\n",
    "print(\"Number of statements: \", len(all_statements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Provide summary statistics (mean, standard deviation, minimum, first quartile, median, third quartile, maximum) for the number of words in each statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for the Number of Words in FOMC Statements:\n",
      "Mean: 1112.0561224489795\n",
      "Standard Deviation: 546.4454667358578\n",
      "Minimum: 160\n",
      "First Quartile: 334.0\n",
      "Median: 1208.5\n",
      "Third Quartile: 1531.75\n",
      "Maximum: 1950\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store word counts for each statement\n",
    "word_counts = []\n",
    "\n",
    "# Loop through the list of statement URLs, download the statements, and count words\n",
    "for statement_url in all_statements:\n",
    "    statement_response = requests.get(statement_url)\n",
    "    statement_soup = BeautifulSoup(statement_response.text, 'html.parser')\n",
    "    statement_text = statement_soup.get_text()\n",
    "    word_count = count_words(statement_text)\n",
    "    word_counts.append(word_count)\n",
    "\n",
    "# Calculate summary statistics\n",
    "mean, stdev, minimum, q1, median, q3, maximum = calculate_summary_statistics(word_counts)\n",
    "\n",
    "# Print the summary statistics\n",
    "print(\"Summary Statistics for the Number of Words in FOMC Statements:\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {stdev}\")\n",
    "print(f\"Minimum: {minimum}\")\n",
    "print(f\"First Quartile: {q1}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Third Quartile: {q3}\")\n",
    "print(f\"Maximum: {maximum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Use the methodology described in section 3.1 (pages 4-8) of Tadle (2022) to measure the tone of each speech.2 Download the Fed Funds Effective Rate from https://fred.stlouisfed.org/series/DFF. Plot both both the statement tone and the Fed Funds Effective Rate over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Comment on the Tadle (2022) methodology. What do you like about it? What are its shortcomings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Describe and implement a different way to measure hawkish/dovish tone of FOMC statements. How does your alternative measure address some of the shortcomings in the Tadle (2022) method? What is the correlation between the Tadle (2022) measure and your measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Redo the plot from problem 2, adding your tone measure. Your plot should include the Tadle (2022) measure, your measure and the Fed Funds Effective Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Complete this problem twice: once with the Tadle (2022) measure of hawkishness and once with your measure. The steps will guide you through using the Fama and MacBeth (1973) procedure to estimate the monetary policy risk premium in industry returns data. In particular:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Estimate an AR(1) model from the hawkishness data. Compute the residual. We will call this the “text-based monetary policy shock.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Use the value-weighted returns from the daily industry returns file for this problem. For each of the 49 industries, regress returns on the day of the Fed announcement on the text-based monetary policy shock. Create a table with three columns: column 1 has the industry name, column 2 has the OLS regression coefficient and column 3 has the p-value for that coefficient. Sort the table from largest to smallest coefficient. To be concrete, the regression you are running at this stage is:\n",
    "\n",
    "$ R_{it} = \\alpha_{i} + \\beta_{i} \\Delta H A W K_{t} + \\epsilon_{it} $\n",
    "\n",
    "for each of the 49 industries, indexed by $i$. Time $t$ here indexes Fed announcement days and $ \\Delta H A W K_{t} $ is the text-based monetary policy shock at time $ t $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Comment on the ordering of the industries. Is it in line with what you would have expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Now turn to the monthly returns data. Again use the value-weighted returns. Separately for each month, regress returns of each industry on its “beta” from step (b). To be concrete, for each month, indexed by $T$, you are running the following regression:\n",
    "\n",
    "$ R_{i,T} = \\gamma_{T} + \\lambda_{T} \\beta_{i} + \\eta_{iT} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) What is the average $\\lambda$ across all months? This is the risk premium associated with holding assets exposed to\n",
    "monetary policy risk. Comment on how its sign can be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) What is the standard deviation of $\\lambda$? Use this to compute the $t-statistic$. Is the risk premium significantly different from zero at the 10%, 5% or 1% level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Response (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional for extra credit. Must be completed individually.\n",
    "\n",
    "Read Aruoba and Drechsel (2022) and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the natural language processing technique that the authors use. What do you like about it? What are its shortcomings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Summarize (in your own words) the findings discussed in Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What questions do you have after reading the paper?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
